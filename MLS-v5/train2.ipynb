{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RND = 123\n",
    "np.random.seed(RND)\n",
    "import random\n",
    "random.seed(RND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN = 'D'\n",
    "MODELS_DIR = '/d3/caches/kaggle-mls-v5/models/' + RUN\n",
    "TFB_DIR = '/tmp-persistent/mls5/' + RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOP_LEN 942\n"
     ]
    }
   ],
   "source": [
    "VAL_SIZE = 0.1\n",
    "\n",
    "MSGS_CACHE_DIR = 'out/msgs-256-processed'\n",
    "\n",
    "SAMPLING_RATE = 400\n",
    "N_SAMPLES = 240000\n",
    "N_CHANNELS = 16\n",
    "\n",
    "N_FFT=512\n",
    "N_MELS=256\n",
    "DESIRED_MSG_W = 256\n",
    "HOP_LEN = 1 + int(N_SAMPLES / (DESIRED_MSG_W - 1))\n",
    "\n",
    "WARM_CACHE = False\n",
    "\n",
    "print 'HOP_LEN', HOP_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure output dirs exist\n",
    "for v in [MSGS_CACHE_DIR, MODELS_DIR, TFB_DIR]: \n",
    "    if not os.path.isdir(v): \n",
    "        os.makedirs(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load inout files list\n",
    "input_df = pd.read_csv('out/input_files.csv', index_col='file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>path</th>\n",
       "      <th>patient_1</th>\n",
       "      <th>patient_2</th>\n",
       "      <th>patient_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2_1288_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_1288_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_725_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_725_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_8_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_1/1_8_0.mat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_1965_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_1965_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_135_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_135_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_2_706.mat</th>\n",
       "      <td>-1</td>\n",
       "      <td>/datasets/kaggle/mls/new/test_2_new/new_2_706.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1661_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_1661_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_1640_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_1640_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_2_233.mat</th>\n",
       "      <td>-1</td>\n",
       "      <td>/datasets/kaggle/mls/new/test_2_new/new_2_233.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_53_1.mat</th>\n",
       "      <td>1</td>\n",
       "      <td>/datasets/kaggle/mls/train_1/1_53_1.mat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_833_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_833_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1152_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_1152_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_1038_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_1/1_1038_0.mat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1744_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_1744_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_758_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_1/1_758_0.mat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_707_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_707_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_865_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_865_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_2050_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_2050_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_3_212.mat</th>\n",
       "      <td>-1</td>\n",
       "      <td>/datasets/kaggle/mls/new/test_3_new/new_3_212.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_294_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_294_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_2233_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_2233_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1622_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_1622_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_1567_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_1567_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_1_215.mat</th>\n",
       "      <td>-1</td>\n",
       "      <td>/datasets/kaggle/mls/new/test_1_new/new_1_215.mat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_126_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_1/1_126_0.mat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_959_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_1/1_959_0.mat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1399_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_1399_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_244_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_244_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_1_34.mat</th>\n",
       "      <td>-1</td>\n",
       "      <td>/datasets/kaggle/mls/new/test_1_new/new_1_34.mat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_666_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_666_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1321_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_1321_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_2_797.mat</th>\n",
       "      <td>-1</td>\n",
       "      <td>/datasets/kaggle/mls/new/test_2_new/new_2_797.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_42_1.mat</th>\n",
       "      <td>1</td>\n",
       "      <td>/datasets/kaggle/mls/train_1/1_42_1.mat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_396_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_396_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_2209_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_2209_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_592_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_592_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_2_970.mat</th>\n",
       "      <td>-1</td>\n",
       "      <td>/datasets/kaggle/mls/new/test_2_new/new_2_970.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_1235_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_1235_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_89_1.mat</th>\n",
       "      <td>1</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_89_1.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_508_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_508_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_3_565.mat</th>\n",
       "      <td>-1</td>\n",
       "      <td>/datasets/kaggle/mls/new/test_3_new/new_3_565.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_101_1.mat</th>\n",
       "      <td>1</td>\n",
       "      <td>/datasets/kaggle/mls/train_1/1_101_1.mat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_144_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_144_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_762_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_762_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_2_623.mat</th>\n",
       "      <td>-1</td>\n",
       "      <td>/datasets/kaggle/mls/new/test_2_new/new_2_623.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_342_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_342_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1174_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_1174_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_420_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_420_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_2_452.mat</th>\n",
       "      <td>-1</td>\n",
       "      <td>/datasets/kaggle/mls/new/test_2_new/new_2_452.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1846_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_1846_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_472_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_472_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_1507_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_1507_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_1719_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_1719_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_85_1.mat</th>\n",
       "      <td>1</td>\n",
       "      <td>/datasets/kaggle/mls/train_1/1_85_1.mat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1927_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_1927_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_292_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_2/2_292_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_127_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_127_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_715_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_715_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1020_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_1020_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_1726_0.mat</th>\n",
       "      <td>0</td>\n",
       "      <td>/datasets/kaggle/mls/train_3/3_1726_0.mat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6672 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               class                                               path  \\\n",
       "file                                                                      \n",
       "2_1288_0.mat       0          /datasets/kaggle/mls/train_2/2_1288_0.mat   \n",
       "3_725_0.mat        0           /datasets/kaggle/mls/train_3/3_725_0.mat   \n",
       "1_8_0.mat          0             /datasets/kaggle/mls/train_1/1_8_0.mat   \n",
       "2_1965_0.mat       0          /datasets/kaggle/mls/train_2/2_1965_0.mat   \n",
       "2_135_0.mat        0           /datasets/kaggle/mls/train_2/2_135_0.mat   \n",
       "new_2_706.mat     -1  /datasets/kaggle/mls/new/test_2_new/new_2_706.mat   \n",
       "3_1661_0.mat       0          /datasets/kaggle/mls/train_3/3_1661_0.mat   \n",
       "2_1640_0.mat       0          /datasets/kaggle/mls/train_2/2_1640_0.mat   \n",
       "new_2_233.mat     -1  /datasets/kaggle/mls/new/test_2_new/new_2_233.mat   \n",
       "1_53_1.mat         1            /datasets/kaggle/mls/train_1/1_53_1.mat   \n",
       "3_833_0.mat        0           /datasets/kaggle/mls/train_3/3_833_0.mat   \n",
       "3_1152_0.mat       0          /datasets/kaggle/mls/train_3/3_1152_0.mat   \n",
       "1_1038_0.mat       0          /datasets/kaggle/mls/train_1/1_1038_0.mat   \n",
       "3_1744_0.mat       0          /datasets/kaggle/mls/train_3/3_1744_0.mat   \n",
       "1_758_0.mat        0           /datasets/kaggle/mls/train_1/1_758_0.mat   \n",
       "2_707_0.mat        0           /datasets/kaggle/mls/train_2/2_707_0.mat   \n",
       "2_865_0.mat        0           /datasets/kaggle/mls/train_2/2_865_0.mat   \n",
       "2_2050_0.mat       0          /datasets/kaggle/mls/train_2/2_2050_0.mat   \n",
       "new_3_212.mat     -1  /datasets/kaggle/mls/new/test_3_new/new_3_212.mat   \n",
       "3_294_0.mat        0           /datasets/kaggle/mls/train_3/3_294_0.mat   \n",
       "3_2233_0.mat       0          /datasets/kaggle/mls/train_3/3_2233_0.mat   \n",
       "3_1622_0.mat       0          /datasets/kaggle/mls/train_3/3_1622_0.mat   \n",
       "2_1567_0.mat       0          /datasets/kaggle/mls/train_2/2_1567_0.mat   \n",
       "new_1_215.mat     -1  /datasets/kaggle/mls/new/test_1_new/new_1_215.mat   \n",
       "1_126_0.mat        0           /datasets/kaggle/mls/train_1/1_126_0.mat   \n",
       "1_959_0.mat        0           /datasets/kaggle/mls/train_1/1_959_0.mat   \n",
       "3_1399_0.mat       0          /datasets/kaggle/mls/train_3/3_1399_0.mat   \n",
       "2_244_0.mat        0           /datasets/kaggle/mls/train_2/2_244_0.mat   \n",
       "new_1_34.mat      -1   /datasets/kaggle/mls/new/test_1_new/new_1_34.mat   \n",
       "2_666_0.mat        0           /datasets/kaggle/mls/train_2/2_666_0.mat   \n",
       "...              ...                                                ...   \n",
       "3_1321_0.mat       0          /datasets/kaggle/mls/train_3/3_1321_0.mat   \n",
       "new_2_797.mat     -1  /datasets/kaggle/mls/new/test_2_new/new_2_797.mat   \n",
       "1_42_1.mat         1            /datasets/kaggle/mls/train_1/1_42_1.mat   \n",
       "3_396_0.mat        0           /datasets/kaggle/mls/train_3/3_396_0.mat   \n",
       "3_2209_0.mat       0          /datasets/kaggle/mls/train_3/3_2209_0.mat   \n",
       "2_592_0.mat        0           /datasets/kaggle/mls/train_2/2_592_0.mat   \n",
       "new_2_970.mat     -1  /datasets/kaggle/mls/new/test_2_new/new_2_970.mat   \n",
       "2_1235_0.mat       0          /datasets/kaggle/mls/train_2/2_1235_0.mat   \n",
       "2_89_1.mat         1            /datasets/kaggle/mls/train_2/2_89_1.mat   \n",
       "3_508_0.mat        0           /datasets/kaggle/mls/train_3/3_508_0.mat   \n",
       "new_3_565.mat     -1  /datasets/kaggle/mls/new/test_3_new/new_3_565.mat   \n",
       "1_101_1.mat        1           /datasets/kaggle/mls/train_1/1_101_1.mat   \n",
       "2_144_0.mat        0           /datasets/kaggle/mls/train_2/2_144_0.mat   \n",
       "3_762_0.mat        0           /datasets/kaggle/mls/train_3/3_762_0.mat   \n",
       "new_2_623.mat     -1  /datasets/kaggle/mls/new/test_2_new/new_2_623.mat   \n",
       "2_342_0.mat        0           /datasets/kaggle/mls/train_2/2_342_0.mat   \n",
       "3_1174_0.mat       0          /datasets/kaggle/mls/train_3/3_1174_0.mat   \n",
       "2_420_0.mat        0           /datasets/kaggle/mls/train_2/2_420_0.mat   \n",
       "new_2_452.mat     -1  /datasets/kaggle/mls/new/test_2_new/new_2_452.mat   \n",
       "3_1846_0.mat       0          /datasets/kaggle/mls/train_3/3_1846_0.mat   \n",
       "3_472_0.mat        0           /datasets/kaggle/mls/train_3/3_472_0.mat   \n",
       "2_1507_0.mat       0          /datasets/kaggle/mls/train_2/2_1507_0.mat   \n",
       "2_1719_0.mat       0          /datasets/kaggle/mls/train_2/2_1719_0.mat   \n",
       "1_85_1.mat         1            /datasets/kaggle/mls/train_1/1_85_1.mat   \n",
       "3_1927_0.mat       0          /datasets/kaggle/mls/train_3/3_1927_0.mat   \n",
       "2_292_0.mat        0           /datasets/kaggle/mls/train_2/2_292_0.mat   \n",
       "3_127_0.mat        0           /datasets/kaggle/mls/train_3/3_127_0.mat   \n",
       "3_715_0.mat        0           /datasets/kaggle/mls/train_3/3_715_0.mat   \n",
       "3_1020_0.mat       0          /datasets/kaggle/mls/train_3/3_1020_0.mat   \n",
       "3_1726_0.mat       0          /datasets/kaggle/mls/train_3/3_1726_0.mat   \n",
       "\n",
       "               patient_1  patient_2  patient_3  \n",
       "file                                            \n",
       "2_1288_0.mat         0.0        1.0        0.0  \n",
       "3_725_0.mat          0.0        0.0        1.0  \n",
       "1_8_0.mat            1.0        0.0        0.0  \n",
       "2_1965_0.mat         0.0        1.0        0.0  \n",
       "2_135_0.mat          0.0        1.0        0.0  \n",
       "new_2_706.mat        0.0        1.0        0.0  \n",
       "3_1661_0.mat         0.0        0.0        1.0  \n",
       "2_1640_0.mat         0.0        1.0        0.0  \n",
       "new_2_233.mat        0.0        1.0        0.0  \n",
       "1_53_1.mat           1.0        0.0        0.0  \n",
       "3_833_0.mat          0.0        0.0        1.0  \n",
       "3_1152_0.mat         0.0        0.0        1.0  \n",
       "1_1038_0.mat         1.0        0.0        0.0  \n",
       "3_1744_0.mat         0.0        0.0        1.0  \n",
       "1_758_0.mat          1.0        0.0        0.0  \n",
       "2_707_0.mat          0.0        1.0        0.0  \n",
       "2_865_0.mat          0.0        1.0        0.0  \n",
       "2_2050_0.mat         0.0        1.0        0.0  \n",
       "new_3_212.mat        0.0        0.0        1.0  \n",
       "3_294_0.mat          0.0        0.0        1.0  \n",
       "3_2233_0.mat         0.0        0.0        1.0  \n",
       "3_1622_0.mat         0.0        0.0        1.0  \n",
       "2_1567_0.mat         0.0        1.0        0.0  \n",
       "new_1_215.mat        1.0        0.0        0.0  \n",
       "1_126_0.mat          1.0        0.0        0.0  \n",
       "1_959_0.mat          1.0        0.0        0.0  \n",
       "3_1399_0.mat         0.0        0.0        1.0  \n",
       "2_244_0.mat          0.0        1.0        0.0  \n",
       "new_1_34.mat         1.0        0.0        0.0  \n",
       "2_666_0.mat          0.0        1.0        0.0  \n",
       "...                  ...        ...        ...  \n",
       "3_1321_0.mat         0.0        0.0        1.0  \n",
       "new_2_797.mat        0.0        1.0        0.0  \n",
       "1_42_1.mat           1.0        0.0        0.0  \n",
       "3_396_0.mat          0.0        0.0        1.0  \n",
       "3_2209_0.mat         0.0        0.0        1.0  \n",
       "2_592_0.mat          0.0        1.0        0.0  \n",
       "new_2_970.mat        0.0        1.0        0.0  \n",
       "2_1235_0.mat         0.0        1.0        0.0  \n",
       "2_89_1.mat           0.0        1.0        0.0  \n",
       "3_508_0.mat          0.0        0.0        1.0  \n",
       "new_3_565.mat        0.0        0.0        1.0  \n",
       "1_101_1.mat          1.0        0.0        0.0  \n",
       "2_144_0.mat          0.0        1.0        0.0  \n",
       "3_762_0.mat          0.0        0.0        1.0  \n",
       "new_2_623.mat        0.0        1.0        0.0  \n",
       "2_342_0.mat          0.0        1.0        0.0  \n",
       "3_1174_0.mat         0.0        0.0        1.0  \n",
       "2_420_0.mat          0.0        1.0        0.0  \n",
       "new_2_452.mat        0.0        1.0        0.0  \n",
       "3_1846_0.mat         0.0        0.0        1.0  \n",
       "3_472_0.mat          0.0        0.0        1.0  \n",
       "2_1507_0.mat         0.0        1.0        0.0  \n",
       "2_1719_0.mat         0.0        1.0        0.0  \n",
       "1_85_1.mat           1.0        0.0        0.0  \n",
       "3_1927_0.mat         0.0        0.0        1.0  \n",
       "2_292_0.mat          0.0        1.0        0.0  \n",
       "3_127_0.mat          0.0        0.0        1.0  \n",
       "3_715_0.mat          0.0        0.0        1.0  \n",
       "3_1020_0.mat         0.0        0.0        1.0  \n",
       "3_1726_0.mat         0.0        0.0        1.0  \n",
       "\n",
       "[6672 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split inout files into train/test sets\n",
    "train_df = input_df[input_df['class'] != -1]\n",
    "test_df = input_df[input_df['class'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trainval_files = train_df.index.tolist()\n",
    "X_trainval_patients = np.vstack((train_df['patient_1'], train_df['patient_2'], train_df['patient_3']))\\\n",
    "    .T.astype(np.float32)\n",
    "y_trainval = np.array(train_df['class'], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batch(X_files=None, X_patients=None, y=None, start_ix=0, n_samples=1, \\\n",
    "              silent=True, compute_means=False, means_per_ch=None, div_by=None):\n",
    "    \n",
    "    X_msgs_batch = np.zeros((n_samples, N_MELS, DESIRED_MSG_W, N_CHANNELS), dtype=np.float32)\n",
    "    X_patients_batch = np.zeros((n_samples, X_patients.shape[1]), dtype=np.float32)        \n",
    "    y_batch = np.zeros([n_samples, 2], dtype=np.float32)\n",
    "    \n",
    "    if compute_means:\n",
    "        means = np.zeros([n_samples, N_CHANNELS], dtype=np.float32)\n",
    "    else:\n",
    "        means = None\n",
    "        \n",
    "    r = range(n_samples) if silent else tqdm(range(n_samples))\n",
    "\n",
    "    for i in r:\n",
    "        ii  = (i + start_ix) % len(X_files)\n",
    "        \n",
    "        mat_f = X_files[ii]\n",
    "        mat_cache_fp = MSGS_CACHE_DIR + '/' + mat_f + '.msgs.mem'\n",
    "        \n",
    "        # check if msgs are chached\n",
    "        if not os.path.isfile(mat_cache_fp):\n",
    "\n",
    "            mat_fp = input_df.ix[mat_f]['path']\n",
    "            waves = lib.read_mat(mat_fp)\n",
    "            \n",
    "            msgs = np.zeros((N_CHANNELS, N_MELS, DESIRED_MSG_W), dtype=np.float32)\n",
    "\n",
    "            for ch in range(16):\n",
    "                msgs[ch] = lib.compute_msg(waves[ch], \\\n",
    "                      desired_msg_w=DESIRED_MSG_W, hop_length=HOP_LEN, \\\n",
    "                      n_fft=N_FFT, n_mels=N_MELS, sr=SAMPLING_RATE)\n",
    "                            \n",
    "            # move channel axis, shape is now: (ix, h, w, ch)\n",
    "            msgs_t = np.swapaxes(msgs, 0, 1)\n",
    "            msgs_t = np.swapaxes(msgs_t, 1, 2)\n",
    "\n",
    "            X_msgs_batch[i] = msgs_t\n",
    "            \n",
    "            if not means_per_ch is None:\n",
    "                X_msgs_batch = np.subtract(X_msgs_batch, means_per_ch)\n",
    "\n",
    "            if not div_by is None:\n",
    "                X_msgs_batch = np.divide(X_msgs_batch, div_by)\n",
    "                \n",
    "            # save to cache\n",
    "            X_msgs_batch[i].tofile(mat_cache_fp)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            X_msgs_batch[i] = np.fromfile(mat_cache_fp, dtype=np.float32).\\\n",
    "                reshape((N_MELS, DESIRED_MSG_W, N_CHANNELS))\n",
    "                \n",
    "        # compute means\n",
    "        if compute_means:\n",
    "            means[i] = np.mean(X_msgs_batch.T.reshape(N_CHANNELS, -1), axis=1)\n",
    "\n",
    "        X_patients_batch[i] = X_patients[ii]\n",
    "        y_batch[i] = [1., 0.] if y[ii] else [0., 1.]\n",
    "            \n",
    "    return X_msgs_batch, X_patients_batch, y_batch, means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# warm cache\n",
    "if WARM_CACHE:\n",
    "    \n",
    "    means = []\n",
    "    \n",
    "    means_per_ch = np.load('out/means_per_ch.npy')\n",
    "\n",
    "    X_files = input_df.index.tolist()\n",
    "    X_patients = np.vstack((input_df['patient_1'], input_df['patient_2'], input_df['patient_3']))\\\n",
    "        .T.astype(np.float32)\n",
    "    y = np.zeros((len(X_files), 1), dtype=np.float32)\n",
    "    \n",
    "    start = 0\n",
    "    stop = len(X_files)\n",
    "\n",
    "    for i in tqdm(xrange(start, stop)):\n",
    "        _msgs, _patients, _ys, _means = \\\n",
    "            gen_batch(X_files, X_patients, y, start_ix=i, n_samples=1, \\\n",
    "                      silent=True, compute_means=True, \\\n",
    "                     means_per_ch=means_per_ch, div_by=255.)\n",
    "#         means.append(_means[0])\n",
    "\n",
    "#     means_per_ch = np.mean(means, axis=0)\n",
    "#     np.save('out/means_per_ch.npy', means_per_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_N_PER_BATCH = 16\n",
    "TRAIN_N_EPOCHS = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_ix = 0\n",
    "    \n",
    "# training data generator\n",
    "def train_generator():\n",
    "\n",
    "    global start_ix\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        b = gen_batch(\n",
    "            X_train_files, X_train_patients, y_train, \n",
    "            start_ix=start_ix, n_samples=TRAIN_N_PER_BATCH\n",
    "        )\n",
    "                \n",
    "        start_ix += TRAIN_N_SAMPLES_PER_EPOCH\n",
    "        \n",
    "        yield [b[0], b[1]], b[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_msg = Input(shape=(N_MELS, DESIRED_MSG_W, N_CHANNELS), name='input_msg')\n",
    "\n",
    "x = Convolution2D(64, 3, 3, border_mode='same', activation='relu')(input_msg)\n",
    "x = Convolution2D(64, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = Convolution2D(128, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = Convolution2D(128, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = Convolution2D(256, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = Convolution2D(256, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = Convolution2D(512, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = Convolution2D(512, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = Convolution2D(512, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = Convolution2D(512, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "x = Convolution2D(512, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = Convolution2D(512, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "conv_out = Flatten()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_patient = Input(shape=(3,), name='input_patient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = merge([conv_out, input_patient], mode='concat')\n",
    "x = Dense(2, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(input=[input_msg, input_patient], output=[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_msg (InputLayer)           (None, 256, 256, 16)  0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 256, 256, 64)  9280        input_msg[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 256, 256, 64)  36928       convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 128, 128, 64)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 128, 128) 73856       maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 128, 128) 147584      convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 64, 64, 128)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 64, 64, 256)   295168      maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 64, 64, 256)   590080      convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 32, 32, 256)   0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 32, 32, 512)   1180160     maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 32, 32, 512)   2359808     convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 16, 16, 512)   0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 16, 16, 512)   2359808     maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 16, 16, 512)   2359808     convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 8, 8, 512)     0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 8, 8, 512)     2359808     maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 8, 8, 512)     2359808     convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 4, 4, 512)     0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 8192)          0           maxpooling2d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "input_patient (InputLayer)       (None, 3)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 8195)          0           flatten_1[0][0]                  \n",
      "                                                                   input_patient[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2)             16392       merge_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 14148488\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "scores = []\n",
    "\n",
    "def score_auc():\n",
    "    s = 0\n",
    "    n = len(X_val_msgs)\n",
    "    y_p = model.predict([X_val_msgs[s:s+n],\n",
    "                         X_val_patients[s:s+n]], \n",
    "                        verbose=False)\n",
    "    return metrics.roc_auc_score(y_val[s:s+n].T[0], y_p.T[0])\n",
    "\n",
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def _validate(self):\n",
    "        s = score_auc()\n",
    "        scores.append(s)\n",
    "        print \"\\n\\n AUC = %.5f\\n\"%s; time.sleep(.5)\n",
    "    def on_train_begin(self, epoch, logs={}):\n",
    "        self._validate()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self._validate()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def cb_shuffle_train_data(batch, logs):\n",
    "    global X_train_files, X_train_patients, y_train\n",
    "    X_train_files, X_train_patients, y_train = \\\n",
    "        shuffle(X_train_files, X_train_patients, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4764/4764 [01:00<00:00, 78.61it/s] \n"
     ]
    }
   ],
   "source": [
    "# generate trainval set\n",
    "X_trainval_msgs, X_trainval_patients, y_trainval, _means = \\\n",
    "    gen_batch(X_trainval_files, X_trainval_patients, y_trainval, \\\n",
    "                n_samples=len(X_trainval_files), silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory for trainval set: 18.61G\n"
     ]
    }
   ],
   "source": [
    "print 'Memory for trainval set: %.2fG' % (X_trainval_msgs.size * 4. / pow(2, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_msgs, X_val_msgs, X_train_patients, X_val_patients, y_train, y_val = \\\n",
    "    train_test_split(X_trainval_msgs, X_trainval_patients, y_trainval, random_state=RND, test_size=VAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6616"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_trainval_msgs\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4287 samples, validate on 477 samples\n",
      "\n",
      "\n",
      " AUC = 0.51516\n",
      "\n",
      "Epoch 1/111\n",
      "4272/4287 [============================>.] - ETA: 0s - loss: 0.3687 - acc: 0.9064\n",
      "\n",
      " AUC = 0.51516\n",
      "\n",
      "4287/4287 [==============================] - 138s - loss: 0.3683 - acc: 0.9065 - val_loss: 0.3299 - val_acc: 0.8973\n",
      "Epoch 2/111\n",
      "4272/4287 [============================>.] - ETA: 0s - loss: 0.3060 - acc: 0.9061\n",
      "\n",
      " AUC = 0.51516\n",
      "\n",
      "4287/4287 [==============================] - 135s - loss: 0.3052 - acc: 0.9065 - val_loss: 0.3322 - val_acc: 0.8973\n",
      "Epoch 3/111\n",
      "4272/4287 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9071\n",
      "\n",
      " AUC = 0.51516\n",
      "\n",
      "4287/4287 [==============================] - 135s - loss: 0.3040 - acc: 0.9065 - val_loss: 0.3301 - val_acc: 0.8973\n",
      "Epoch 4/111\n",
      " 672/4287 [===>..........................] - ETA: 108s - loss: 0.2997 - acc: 0.9077"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b043f893da9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;34m'/e{epoch:02d}-l={loss:.5f}-vl={val_loss:.5f}-a={acc:.5f}-va={val_acc:.5f}.h5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             )\n\u001b[1;32m     20\u001b[0m         ]\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.1.0-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1117\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.1.0-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    835\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.1.0-py2.7.egg/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "hist = model.fit(\n",
    "        [X_train_msgs, X_train_patients],\n",
    "        y_train,\n",
    "        batch_size=TRAIN_N_PER_BATCH,\n",
    "        nb_epoch=TRAIN_N_EPOCHS,\n",
    "#         validation_split=0.2,\n",
    "        validation_data=([X_val_msgs, X_val_patients], y_val),\n",
    "        verbose=True,\n",
    "        shuffle=True,\n",
    "        callbacks = [\n",
    "            MyCallback(),\n",
    "            TensorBoard(log_dir=TFB_DIR, histogram_freq=0),\n",
    "            ModelCheckpoint(\n",
    "                MODELS_DIR + \\\n",
    "                '/e{epoch:02d}-l={loss:.5f}-vl={val_loss:.5f}-a={acc:.5f}-va={val_acc:.5f}.h5', \n",
    "                monitor='val_acc', verbose=0, save_best_only=False, \n",
    "                save_weights_only=False, mode='auto'\n",
    "            )\n",
    "        ]\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(X_trainval_patients)):\n",
    "    X_trainval_patients[i,:] = y_trainval[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainval_patients[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainval[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_msg\n",
      "convolution2d_1\n",
      "convolution2d_2\n",
      "maxpooling2d_1\n",
      "convolution2d_3\n",
      "convolution2d_4\n",
      "maxpooling2d_2\n",
      "convolution2d_5\n",
      "convolution2d_6\n",
      "maxpooling2d_3\n",
      "convolution2d_7\n",
      "convolution2d_8\n",
      "maxpooling2d_4\n",
      "convolution2d_9\n",
      "convolution2d_10\n",
      "maxpooling2d_5\n"
     ]
    }
   ],
   "source": [
    "for l in model.layers[:16]:\n",
    "    print l.name\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
